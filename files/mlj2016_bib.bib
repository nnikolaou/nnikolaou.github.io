@Article{Nikolaou2016,
author="Nikolaou, Nikolaos
and Edakunni, Narayanan
and Kull, Meelis
and Flach, Peter
and Brown, Gavin",
title="Cost-sensitive boosting algorithms: Do we really need them?",
journal="Machine Learning",
year="2016",
volume="104",
number="2",
pages="359--384",
abstract="We provide a unifying perspective for two decades of work on cost-sensitive Boosting algorithms. When analyzing the literature 1997--2016, we find 15 distinct cost-sensitive variants of the original algorithm; each of these has its own motivation and claims to superiority---so who should we believe? In this work we critique the Boosting literature using four theoretical frameworks: Bayesian decision theory, the functional gradient descent view, margin theory, and probabilistic modelling. Our finding is that only three algorithms are fully supported---and the probabilistic model view suggests that all require their outputs to be calibrated for best performance. Experiments on 18 datasets across 21 degrees of imbalance support the hypothesis---showing that once calibrated, they perform equivalently, and outperform all others. Our final recommendation---based on simplicity, flexibility and performance---is to use the original Adaboost algorithm with a shifted decision threshold and calibrated probability estimates.",
issn="1573-0565",
doi="10.1007/s10994-016-5572-x",
url="http://dx.doi.org/10.1007/s10994-016-5572-x"
}